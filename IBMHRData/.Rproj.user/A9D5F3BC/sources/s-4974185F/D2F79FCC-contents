if("pacman" %in% rownames(installed.packages()) == FALSE) {install.packages("pacman")}
pacman::p_load("caret", "dplyr", "e1071", "glmnet","lift","MASS", "ROCR", "partykit", "pracma","xgboost", "Hmisc", "pastecs", "psych")
library(stringr)
library(dplyr)

source("helper_functions.r")

##
## Fixing bad data
##

data <- read.csv("IBMHRData.csv", na.strings=c(""," ","NA"), header=TRUE) # Loading data
data <- clean(data)

### Analyze clusters
# files <- c("ProjectData_with_hclust_membership_all_factors.csv",
#             "ProjectData_with_hclust_membership_compressed_factors.csv",
#             "ProjectData_with_kmeans_membership_all_factors.csv",
#             "ProjectData_with_kmeans_membership_compressed_factors.csv")
# for (file in files) {
#   data <- read.csv(file, na.strings=c(""," ","NA"), header=TRUE) # Loading data
#   data <- clean(data)
#   desc <- describeBy(data, data$cluster, mat = TRUE)
#   write.csv(desc, paste(file, "_RESULTS.csv"))
# }

##
## Separating the data
##

training_pct <- 0.8
size_all <- nrow(data)
size_training <- size_all * training_pct

set.seed(110392) 
inTrain <- createDataPartition(y = data$Is_Resigning,
                               p = training_pct, list = FALSE)
training <- data[ inTrain,]
validation_and_testing <- data[ -inTrain,]

inTest <- createDataPartition(y = validation_and_testing$Is_Resigning, p = 0.5, list = FALSE)

validation <- validation_and_testing[ -inTest,]

# leave alone until very end
testing <- validation_and_testing[ inTest,]

##
## Defining the model using logistic (previously used stepAIC)
#

model <- glm(Is_Resigning ~ YearsInCurrentRole + MonthlyIncome + StockOptionLevel + MaritalStatus + JobRole + EducationField + EnvironmentSatisfaction + PerformanceRating + Education + OverTime + BusinessTravel + JobInvolvement + WorkLifeBalance + MonthlyRate, data=training, family="binomial"(link="logit"))
model <- stepAIC(model,direction = c("both"),trace = 1)
ctree_tree<-ctree(Is_Resigning~.,data=training)
plot(ctree_tree, gp = gpar(fontsize = 8)) #Plotting the tree (adjust fontsize if needed)

ctree_probabilities<-predict(ctree_tree,newdata=validation,type="prob") 


##
## Prediction of validation data
##
probabilities <- ctree_probabilities[,2]
#probabilities <- predict(model, newdata=validation, type="response")
classification <- rep("1", nrow(validation))

avg_probability <- mean(training$Is_Resigning == "1")
classification[probabilities < avg_probability] = "0"
classification <- as.factor(classification)
confusionMatrix(classification,validation$Is_Resigning, positive = "1")

# ROC Curve
ROC_prediction <- prediction(probabilities, validation$Is_Resigning)
ROC <- performance(ROC_prediction,"tpr","fpr") # Create ROC curve data
plot(ROC) # Plot ROC curve

# AUC (area under curve)
    # 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
auc.tmp <- performance(ROC_prediction,"auc") # Create AUC data
auc_validation <- as.numeric(auc.tmp@y.values) # Calculate AUC
auc_validation

# Lift chart
plotLift(probabilities, validation$Is_Resigning, cumulative = TRUE, n.buckets = 10)

##
## Prediction of testing data - Currently logistic
##

# Prediction
probabilities <- predict(model, newdata=testing, type="response")

# Classification - Will they resign?
classification <- rep("1", nrow(testing))
avg_probability <- mean(training$Is_Resigning == "1")
classification[probabilities < avg_probability] = "0"
classification <- as.factor(classification)

confusionMatrix(classification,testing$Is_Resigning, positive = "1")

# ROC Curve
ROC_prediction <- prediction(probabilities, testing$Is_Resigning)
ROC <- performance(ROC_prediction,"tpr","fpr") # Create ROC curve data
plot(ROC) # Plot ROC curve

# AUC (area under curve)
# 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
auc.tmp <- performance(ROC_prediction,"auc") # Create AUC data
auc_testing <- as.numeric(auc.tmp@y.values) # Calculate AUC
auc_testing

# Lift chart
plotLift(probabilities, testing$Is_Resigning, cumulative = TRUE, n.buckets = 10)
